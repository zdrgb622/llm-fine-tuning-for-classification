{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bfcb25c-8bb2-41fa-90f9-755a8c1a67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments,\n",
    "AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList, BitsAndBytesConfig, pipeline)\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output \n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a158bfb6-2e63-43c4-baa7-a1d6716ef857",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae08e51c-f940-4d7a-994b-da44f58f18a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cd7d2fcff840748de79ffd4bc6d835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the model\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Some models don't have a dedicated padding token, so using the EOS token as a pad token is a common practice\n",
    "tokenizer.padding_side = \"right\" # configures the tokenizer to add padding to the right side of the sequences\n",
    "\n",
    "# quantization_config\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True,  # model is quantizied to 4bit format\n",
    "                                  bnb_4bit_quant_type=\"nf4\",  # using normalized float 4 method to quantizied\n",
    "                                  bnb_4bit_compute_dtype=\"float16\",  # computing with 16 bit float\n",
    "                                  bnb_4bit_use_double_quant=False)  # no double quantizie\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config = quant_config,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"./fine_tuned_model-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c09047-b1fe-423e-92ba-b0dca203b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def text_for_input(user_prompt=None, system_prompt=None):\n",
    "    user_prompt_template=\"\"\"\\\n",
    "<|begin_of_text|>\\\n",
    "<|start_header_id|>user<|end_header_id|>\\\n",
    "{user_prompt} \\\n",
    "<|eot_id|>\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\\n",
    "\"\"\"\n",
    "    system_prompt_template=\"\"\"\\\n",
    "<|begin_of_text|>\\\n",
    "<|start_header_id|>system<|end_header_id|>\\\n",
    "{system_prompt} \\\n",
    "<|eot_id|>\\\n",
    "<|start_header_id|>user<|end_header_id|>\\\n",
    "{user_prompt}\\\n",
    "<|eot_id|>\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\\n",
    "\"\"\"\n",
    "    if user_prompt != None and system_prompt == None:\n",
    "        prompt_object = PromptTemplate(template=user_prompt_template)\n",
    "        return prompt_object.format(user_prompt=user_prompt)\n",
    "    elif user_prompt != None and system_prompt != None:\n",
    "        prompt_object = PromptTemplate(template=system_prompt_template)\n",
    "        return prompt_object.format(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "    return \"no input\"\n",
    "\n",
    "system_for_fine_tune=\"\"\"\\\n",
    "You are a classification assistant tasked with categorizing a given domain name into one of three labels: normal, dictionary, or random.\n",
    "Definitions:\n",
    "Normal: A legitimate, standard domain name (e.g., google.com, meta.com).\n",
    "Dictionary: A domain generated algorithmically using dictionary words. Examples:\n",
    "    Simple: daylikesheltersoildistrict.com\n",
    "    More advanced: availablesolutions.com\n",
    "Random: A domain generated algorithmically as a random string of characters (e.g., wwmwqiqsicmgicwg.org).\n",
    "Instructions:\n",
    "Your response must contain only one of the following labels:\n",
    "    normal for legitimate domains\n",
    "    dictionary for dictionary-based domain generation algorithms domains\n",
    "    random for randomly generated domain generation algorithms domains\n",
    "Do not include any additional text or commentary.\n",
    "\"\"\"\n",
    "# load the data\n",
    "dataset_name = \"data/balanced_record_without_llm.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files=dataset_name)\n",
    "\n",
    "prompt_template = text_for_input(user_prompt=\"{domain}\", system_prompt=system_for_fine_tune) + \"{label}<|eot_id|>\"\n",
    "prompt_template = PromptTemplate(template=prompt_template)\n",
    "def new_output(example):\n",
    "    if example[\"default/class\"] == 0:\n",
    "        return{\"new_class\": \"normal\"}\n",
    "    elif example[\"default/class\"] == 1:\n",
    "        return{\"new_class\": \"random\"}\n",
    "    else:\n",
    "        return{\"new_class\": \"dictionary\"}\n",
    "\n",
    "def create_prompt(example):\n",
    "    return {\n",
    "        \"prompt\": prompt_template.format(\n",
    "            domain=example[\"default/domain\"],\n",
    "            label=example[\"new_class\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def tokenize_and_mask(example):\n",
    "    full_text = example[\"prompt\"]\n",
    "    # we only want to know the part that assistant output\n",
    "    assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    marker_index = full_text.find(assistant_marker)\n",
    "    if marker_index == -1:\n",
    "        raise ValueError(\"Assistant marker not found in prompt.\")\n",
    "    \n",
    "    prompt_part = full_text[:marker_index + len(assistant_marker)]\n",
    "    \n",
    "    tokenized = tokenizer(full_text, truncation=True, max_length=200, padding=\"max_length\")\n",
    "    \n",
    "    prompt_tokenized = tokenizer(prompt_part, truncation=True, max_length=200)\n",
    "    prompt_length = len(prompt_tokenized[\"input_ids\"])\n",
    "    \n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels[:prompt_length] = [-100] * prompt_length\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "def mask_answer(prompt_text):\n",
    "    assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    if assistant_marker in prompt_text:\n",
    "        return prompt_text.split(assistant_marker)[0] + assistant_marker\n",
    "    else:\n",
    "        return prompt_text\n",
    "def predict_fn(example):\n",
    "    test_input = example['test_input']\n",
    "    max_length = len(tokenizer(test_input)['input_ids']) + 1\n",
    "    max_new_tokens = max_length\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length, logits_processor=logits_processor, max_new_tokens= max_new_tokens)\n",
    "    result = pipe(test_input)\n",
    "    \n",
    "    match = re.search(r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(\\S+)\", result[0]['generated_text'])\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    return {\"predict\": match.group(1) if match else 'error'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ff7c4e-5d59-4ec1-89ff-48f958bb3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map is the apply in pd\n",
    "new_dataset = dataset.map(new_output)\n",
    "new_dataset = new_dataset.map(create_prompt)\n",
    "new_dataset = new_dataset.map(tokenize_and_mask)\n",
    "new_dataset = new_dataset.map(lambda x: {\"test_input\": mask_answer(x[\"prompt\"])})\n",
    "# new_dataset = new_dataset.remove_columns(['default/domain', 'default/label', 'default/class', 'new_class',])\n",
    "# train test splite\n",
    "splited_dataset = new_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = splited_dataset[\"train\"]\n",
    "test_dataset = splited_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca71090-b276-4e25-b487-7b781cc00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making model generate fixed output by modifying the output token choice\n",
    "class RestrictLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_tokens):\n",
    "        self.allowed_tokens = allowed_tokens  # allowed token\n",
    "    \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # making all token except allowed token to -inf\n",
    "        restricted_scores = torch.full_like(scores, float('-inf'))\n",
    "        restricted_scores[:, self.allowed_tokens] = scores[:, self.allowed_tokens]\n",
    "        return restricted_scores\n",
    "\n",
    "# restrict the model word choice to only these 3 token are allowed to generate\n",
    "allowed_words = [\"normal\", \"random\", \"dictionary\", \"<|eot_id|>\"]\n",
    "allowed_tokens = []\n",
    "for word in allowed_words:\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    allowed_tokens.extend(token_ids)\n",
    "logits_processor = LogitsProcessorList()\n",
    "logits_processor.append(RestrictLogitsProcessor(allowed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b00f7d7-758e-4c4c-8df2-9e284c5f77cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved up to sample 60000\n",
      "All predictions processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "batch_size = 100\n",
    "output_csv = \"dataset_results_llama_without_llm.csv\"\n",
    "\n",
    "if os.path.exists(output_csv):\n",
    "    processed_df = pd.read_csv(output_csv)\n",
    "    processed_count = len(processed_df)\n",
    "    print(f\"Resuming from {processed_count} samples.\")\n",
    "else:\n",
    "    processed_df = pd.DataFrame()\n",
    "    processed_count = 0\n",
    "    print(\"Starting from scratch.\")\n",
    "\n",
    "test_dataset = test_dataset\n",
    "\n",
    "remaining_dataset = test_dataset.select(range(processed_count, len(test_dataset)))\n",
    "\n",
    "for i in range(0, len(remaining_dataset), batch_size):\n",
    "    batch = remaining_dataset.select(range(i, min(i + batch_size, len(remaining_dataset))))\n",
    "    \n",
    "    batch_processed = batch.map(predict_fn)\n",
    "    \n",
    "    batch_df = batch_processed.to_pandas()\n",
    "    processed_df = pd.concat([processed_df, batch_df], ignore_index=True)\n",
    "    \n",
    "    processed_df.to_csv(output_csv, index=False)\n",
    "    current_total = processed_count + i + len(batch)\n",
    "    print(f\"Saved up to sample {current_total}\")\n",
    "    \n",
    "print(\"All predictions processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26b9a1f6-b39f-40d8-8020-661f87a96f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[17778  2043   373]\n",
      " [  433 19249     9]\n",
      " [  347   479 19289]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  dictionary     0.9806    0.9589    0.9696     20115\n",
      "      normal     0.9580    0.8804    0.9175     20194\n",
      "      random     0.8842    0.9776    0.9285     19691\n",
      "\n",
      "    accuracy                         0.9386     60000\n",
      "   macro avg     0.9409    0.9389    0.9386     60000\n",
      "weighted avg     0.9413    0.9386    0.9386     60000\n",
      "\n",
      "Accuracy: 0.9386\n",
      "Precision: 0.9413\n",
      "Recall: 0.9386\n",
      "F1 Score: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# llm generated dataset\n",
    "df = pd.read_csv(\"./data/dataset_results_llama_without_llm.csv\")\n",
    "def clean_label(label):\n",
    "    if \"dictionary\" in label:\n",
    "        return \"dictionary\"\n",
    "    elif \"normal\" in label:\n",
    "        return \"normal\"\n",
    "    elif \"random\" in label:\n",
    "        return \"random\"\n",
    "    else:\n",
    "        return label \n",
    "\n",
    "df['predict'] = df['predict'].apply(clean_label)\n",
    "y_true = df['new_class']\n",
    "y_pred = df['predict']\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['normal', 'random', 'dictionary'])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de925a65-bc7a-4f28-9ecd-4ee0707fce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[    0     0     0]\n",
      " [    0     0     0]\n",
      " [13535   441  3147]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  dictionary     1.0000    0.1838    0.3105     17123\n",
      "      normal     0.0000    0.0000    0.0000         0\n",
      "      random     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.1838     17123\n",
      "   macro avg     0.3333    0.0613    0.1035     17123\n",
      "weighted avg     1.0000    0.1838    0.3105     17123\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\aa\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\aa\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\aa\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1838\n",
      "Precision: 1.0000\n",
      "Recall: 0.1838\n",
      "F1 Score: 0.3105\n"
     ]
    }
   ],
   "source": [
    "# original dataset\n",
    "df = pd.read_csv(\"./data/dataset_llm_generated_by_llama_.csv\")\n",
    "y_true = df['new_class']\n",
    "y_pred = df['predict']\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['normal', 'random', 'dictionary'])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01be2d6-b405-441b-96f4-2c6775ca58f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
